---
title: "Untitled"
format: html
---

# Python Code
```{python}
# load libraries
from datasets import load_dataset
import torch
from torchvision.ops import box_convert
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import pil_to_tensor, to_pil_image
from transformers import pipeline
```

```{python}
# load dataset
ds = load_dataset("Kili/plastic_in_river")
ds
```

Datenset hat 3407 Trainingsdatensätze, 427 für Test und 425 für Validation.



```{python}
train = ds['train']
val = ds['validation']
test = ds['test']

```

```{python}
# set example
example = train[2]
example
```

```{python}
print('image size: ' + str(example['image'].size))
```

```{python}
width = example['image'].size[0]
height = example['image'].size[1]
```

```{python}
# all possible object categories
categories = train.features['litter'].feature['label']
categories
```

```{python}
# resize bbox to image size
def resize_bbox(bbox, w, h):
    for x in range(len(bbox)):
        bbox[x][0] = bbox[x][0] * w
        bbox[x][1] = bbox[x][1] * h
        bbox[x][2] = bbox[x][2] * w
        bbox[x][3] = bbox[x][3] * h
    return bbox
```


```{python}
# show image without bbox
example['image']
```


```{python}
# function to draw image with bounding boxes
def plot_image(example):
    bbox = resize_bbox(example['litter']['bbox'], width, height)
    boxes_xywh = torch.tensor(bbox)
    boxes_xyxy = box_convert(boxes_xywh, 'cxcywh', 'xyxy')
    
    labels = [categories.int2str(x) for x in example['litter']['label']]
    return to_pil_image(
        draw_bounding_boxes(
            pil_to_tensor(example['image']),
            boxes_xyxy,
            colors="red",
            labels=labels,
        )
    )
```

```{python}
plot_image(example)
```

```{python}
from transformers import AutoImageProcessor

checkpoint = "facebook/detr-resnet-50"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

```{python}
pip install -U albumentations opencv-python
```


```{python}
import albumentations
import numpy as np

transform = albumentations.Compose([
    albumentations.Resize(480, 480),
    albumentations.HorizontalFlip(p=1.0),
    albumentations.RandomBrightnessContrast(p=1.0),
], bbox_params=albumentations.BboxParams(format='coco',  label_fields=['category']))

# RGB PIL Image -> BGR Numpy array
image = np.flip(np.array(example['image']), -1)
out = transform(
    image=image,
    bboxes=example['litter']['bbox'],
    category=example['litter']['label'],
)
```

# resize bbox muss davor gemacht werden, sonst wird bbox nicht angezeigt

```{python}
image = torch.tensor(out['image']).flip(-1).permute(2, 0, 1)
boxes_xywh = torch.stack([torch.tensor(x) for x in out['bboxes']])
boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')
labels = [categories.int2str(x) for x in out['category']]
to_pil_image(
    draw_bounding_boxes(
        image,
        boxes_xyxy,
        colors='red',
        labels=labels
    )
)
```


```{python}
boxes_xyxy
```

# Example Dataset From Huggingface 
```{python}
ds2 = load_dataset("cppe-5")
example2 = ds2['train'][0]
example2
```

```{python}
# original code for corona dataset 
categories2 = ds2['train'].features['objects'].feature['category']
boxes_xywh2 = torch.tensor(example2['objects']['bbox'])
boxes_xyxy2 = box_convert(boxes_xywh2, 'xywh', 'xyxy')
labels2 = [categories2.int2str(x) for x in example2['objects']['category']]
to_pil_image(
    draw_bounding_boxes(
        pil_to_tensor(example2['image']),
        boxes_xyxy2,
        colors="red",
        labels=labels2,
    )
)
```


```{python}
ds['train'].features['objects'].feature['category']
```